{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import faiss\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# ====================== Output Folder Setup =======================\n",
    "OUTPUT_DIR = \"new_run\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ====================== Feature Extraction for Multiple Models =======================\n",
    "\n",
    "def extract_features_model(image, model, transform):\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image = image.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))  # Move to GPU if available\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    \n",
    "    return features.cpu().numpy().flatten()\n",
    "\n",
    "# ====================== Load Images from Multiple Directories =======================\n",
    "\n",
    "def load_images_from_directories(directories):\n",
    "    images, filenames = [], []\n",
    "    for directory in directories:\n",
    "        for file_path in glob.glob(os.path.join(directory, '*.jpg')):\n",
    "            img = Image.open(file_path)\n",
    "            images.append(img)\n",
    "            filenames.append(os.path.basename(file_path))\n",
    "    return images, filenames\n",
    "\n",
    "def extract_features_from_dataset(directories, model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    images, filenames = load_images_from_directories(directories)\n",
    "    \n",
    "    features_array = np.array([extract_features_model(img, model, transform) for img in images])\n",
    "    \n",
    "    return features_array, filenames, images\n",
    "\n",
    "# ====================== FAISS Indexing =======================\n",
    "\n",
    "def build_faiss_index(features_array, distance_metric='L2'):\n",
    "    embedding_dim = features_array.shape[1]\n",
    "    \n",
    "    if distance_metric == 'L2':\n",
    "        index = faiss.IndexFlatL2(embedding_dim)\n",
    "    elif distance_metric == 'Cosine':\n",
    "        index = faiss.IndexFlatIP(embedding_dim)\n",
    "        # Normalize features for cosine similarity\n",
    "        features_array_copy = features_array.copy()\n",
    "        faiss.normalize_L2(features_array_copy)\n",
    "        index.add(features_array_copy)\n",
    "        return index\n",
    "    \n",
    "    index.add(features_array)\n",
    "    return index\n",
    "\n",
    "\n",
    "def build_ivf_pq_index(features_array, n_clusters, n_bits):\n",
    "    \"\"\"Build IVF-PQ index with specified parameters for hyperparameter testing\"\"\"\n",
    "    embedding_dim = features_array.shape[1]\n",
    "    \n",
    "    # Create quantizer\n",
    "    quantizer = faiss.IndexFlatL2(embedding_dim)\n",
    "    \n",
    "    # Create IVF-PQ index\n",
    "    index = faiss.IndexIVFPQ(quantizer, embedding_dim, n_clusters, \n",
    "                             n_bits, 8)  # 8 is the number of bits per subquantizer\n",
    "    \n",
    "    # Train and add vectors\n",
    "    index.train(features_array)\n",
    "    index.add(features_array)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def perform_faiss_search(index, query_index, features_array, k=5, distance_metric='L2', n_trials=10):\n",
    "    \"\"\"Perform search with multiple trials to calculate mean and std of search time\"\"\"\n",
    "    query_vector = features_array[query_index:query_index+1].copy()\n",
    "    \n",
    "    if distance_metric == 'Cosine':\n",
    "        faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    # Run multiple trials for timing\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        start_time = time.perf_counter()  # Higher precision timer\n",
    "        distances, indices = index.search(query_vector, k=k)\n",
    "        times.append(time.perf_counter() - start_time)\n",
    "    \n",
    "    search_time_mean = np.mean(times)\n",
    "    search_time_std = np.std(times)\n",
    "    \n",
    "    return distances[0], indices[0], search_time_mean, search_time_std\n",
    "\n",
    "# ====================== Load BIRADS Labels from Excel =======================\n",
    "\n",
    "def load_birads_labels(excel_file):\n",
    "    df = pd.read_excel(excel_file, sheet_name='all')\n",
    "    birads_labels = df['BIRADS'].tolist()\n",
    "    \n",
    "    # Count occurrences of each BIRADS category\n",
    "    category_counts = df['BIRADS'].value_counts().to_dict()\n",
    "    print(f\"BIRADS Category Distribution: {category_counts}\")\n",
    "    \n",
    "    return birads_labels, df['BIRADS']\n",
    "\n",
    "# ====================== Precision, Recall, NDCG =======================\n",
    "\n",
    "def precision_at_k(retrieved_labels, true_label, k=5):\n",
    "    return sum([1 for label in retrieved_labels[:k] if label == true_label]) / k\n",
    "\n",
    "def recall_at_k(retrieved_labels, true_label, total_relevant, k=5):\n",
    "    retrieved_relevant = sum([1 for label in retrieved_labels[:k] if label == true_label])\n",
    "    return retrieved_relevant / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "def ndcg_at_k(retrieved_labels, true_label, k=5):\n",
    "    dcg = sum([1 / np.log2(i + 2) if retrieved_labels[i] == true_label else 0 for i in range(k)])\n",
    "    idcg = sum([1 / np.log2(i + 2) for i in range(min(k, sum([1 for l in retrieved_labels if l == true_label])))])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def rank_of_true_label(retrieved_labels, true_label):\n",
    "    try:\n",
    "        return retrieved_labels.index(true_label) + 1\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# ====================== PCA and t-SNE Visualization =======================\n",
    "\n",
    "def visualize_pca_and_tsne(features_array, labels, model_name, n_points=500):\n",
    "    # Convert labels to numpy array for easier handling\n",
    "    labels_array = np.array(labels[:n_points])\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=50)\n",
    "    reduced_pca = pca.fit_transform(features_array[:n_points])\n",
    "    \n",
    "    # Calculate and print explained variance\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    print(f\"Explained variance ratio by PCA components for {model_name}: {explained_variance:.2f}\")\n",
    "    \n",
    "    # First 2 components for PCA visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique labels and create a categorical colormap\n",
    "    unique_labels = np.unique(labels_array)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    # Create scatter plot with clear label distinctions\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = np.where(labels_array == label)[0]\n",
    "        plt.scatter(reduced_pca[idx, 0], reduced_pca[idx, 1], \n",
    "                   color=colors[i], label=f'BIRADS {int(label)}', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.2f})\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.2f})\")\n",
    "    plt.title(f\"PCA Visualization of {model_name} Feature Space\")\n",
    "    plt.legend(title='BIRADS Category')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'pca_{model_name}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform t-SNE on the PCA reduced data\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_tsne = tsne.fit_transform(reduced_pca)\n",
    "    \n",
    "    # Create t-SNE visualization with clear label distinctions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        idx = np.where(labels_array == label)[0]\n",
    "        plt.scatter(reduced_tsne[idx, 0], reduced_tsne[idx, 1], \n",
    "                   color=colors[i], label=f'BIRADS {int(label)}', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"t-SNE Visualization of {model_name} Embedding Space (after PCA)\")\n",
    "    plt.legend(title='BIRADS Category')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,f\"tsne_{model_name}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced_pca, reduced_tsne\n",
    "\n",
    "# ====================== Distance Metric Analysis =======================\n",
    "\n",
    "def analyze_distance_metrics(features_array, birads_labels, model_name, query_indices, k_values):\n",
    "    \"\"\"Compare L2 distance vs Cosine similarity\"\"\"\n",
    "    distance_metrics = ['L2', 'Cosine']\n",
    "    results = []\n",
    "    \n",
    "    for distance_metric in distance_metrics:\n",
    "        print(f\"\\nAnalyzing {distance_metric} distance for {model_name}...\")\n",
    "        \n",
    "        # Build index\n",
    "        index = build_faiss_index(features_array, distance_metric)\n",
    "        \n",
    "        for query_index in query_indices:\n",
    "            true_label = birads_labels[query_index]\n",
    "            total_relevant = birads_labels.count(true_label)\n",
    "            \n",
    "            for k in k_values:\n",
    "                distances, indices, search_time_mean, search_time_std = perform_faiss_search(\n",
    "                    index, query_index, features_array, k=k, \n",
    "                    distance_metric=distance_metric, n_trials=10\n",
    "                )\n",
    "                \n",
    "                retrieved_labels = [birads_labels[idx] for idx in indices if idx < len(birads_labels)]\n",
    "                \n",
    "                precision = precision_at_k(retrieved_labels, true_label, k=k)\n",
    "                recall = recall_at_k(retrieved_labels, true_label, total_relevant, k=k)\n",
    "                ndcg = ndcg_at_k(retrieved_labels, true_label, k=k)\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_name,\n",
    "                    'distance_metric': distance_metric,\n",
    "                    'query_image': query_index,\n",
    "                    'k': k,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'ndcg': ndcg,\n",
    "                    'search_time_mean': search_time_mean,\n",
    "                    'search_time_std': search_time_std,\n",
    "                    'birads_category': true_label\n",
    "                })\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, f'distance_metric_comparison_{model_name}.csv'), index=False)\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Subplot for precision\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for metric in distance_metrics:\n",
    "        metric_data = df[df['distance_metric'] == metric]\n",
    "        for k in k_values:\n",
    "            k_data = metric_data[metric_data['k'] == k]\n",
    "            plt.scatter(k, k_data['precision'].mean(), \n",
    "                      label=f\"{metric} (k={k})\")\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.title(f'Average Precision by Distance Metric ({model_name})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Subplot for recall\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for metric in distance_metrics:\n",
    "        metric_data = df[df['distance_metric'] == metric]\n",
    "        for k in k_values:\n",
    "            k_data = metric_data[metric_data['k'] == k]\n",
    "            plt.scatter(k, k_data['recall'].mean(), \n",
    "                      label=f\"{metric} (k={k})\")\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Average Recall')\n",
    "    plt.title(f'Average Recall by Distance Metric ({model_name})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Subplot for NDCG\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for metric in distance_metrics:\n",
    "        metric_data = df[df['distance_metric'] == metric]\n",
    "        for k in k_values:\n",
    "            k_data = metric_data[metric_data['k'] == k]\n",
    "            plt.scatter(k, k_data['ndcg'].mean(), \n",
    "                      label=f\"{metric} (k={k})\")\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Average NDCG')\n",
    "    plt.title(f'Average NDCG by Distance Metric ({model_name})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Subplot for search time\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for metric in distance_metrics:\n",
    "        metric_data = df[df['distance_metric'] == metric]\n",
    "        for k in k_values:\n",
    "            k_data = metric_data[metric_data['k'] == k]\n",
    "            plt.errorbar(k, k_data['search_time_mean'].mean(), \n",
    "                       yerr=k_data['search_time_std'].mean(),\n",
    "                       label=f\"{metric} (k={k})\")\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Average Search Time (s)')\n",
    "    plt.title(f'Average Search Time by Distance Metric ({model_name})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,f'distance_metric_comparison_{model_name}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====================== Hyperparameter Analysis for FAISS =======================\n",
    "\n",
    "def analyze_faiss_hyperparameters(features_array, birads_labels, model_name, query_indices):\n",
    "    \"\"\"Test different hyperparameters for FAISS indexing\"\"\"\n",
    "    n_clusters_values = [4, 8, 16, 32]\n",
    "    n_bits_values = [4, 8]\n",
    "    results = []\n",
    "    k = 10  # Fixed k for hyperparameter analysis\n",
    "    \n",
    "    for n_clusters in n_clusters_values:\n",
    "        for n_bits in n_bits_values:\n",
    "            print(f\"\\nTesting FAISS with n_clusters={n_clusters}, n_bits={n_bits}...\")\n",
    "            \n",
    "            try:\n",
    "                # Build index with these parameters\n",
    "                index = build_ivf_pq_index(features_array, n_clusters, n_bits)\n",
    "                \n",
    "                # Set nprobe (number of clusters to visit during search)\n",
    "                index.nprobe = max(1, n_clusters // 4)\n",
    "                \n",
    "                for query_index in query_indices:\n",
    "                    true_label = birads_labels[query_index]\n",
    "                    total_relevant = birads_labels.count(true_label)\n",
    "                    \n",
    "                    # Perform search\n",
    "                    start_times = []\n",
    "                    all_distances = []\n",
    "                    all_indices = []\n",
    "                    \n",
    "                    # Run multiple trials\n",
    "                    n_trials = 5\n",
    "                    for _ in range(n_trials):\n",
    "                        start_time = time.perf_counter()\n",
    "                        distances, indices = index.search(features_array[query_index:query_index+1], k=k)\n",
    "                        start_times.append(time.perf_counter() - start_time)\n",
    "                        all_distances.append(distances[0])\n",
    "                        all_indices.append(indices[0])\n",
    "                    \n",
    "                    # Use most frequent indices result\n",
    "                    from collections import Counter\n",
    "                    indices_counter = Counter([tuple(arr) for arr in all_indices])\n",
    "                    most_common_indices = np.array(indices_counter.most_common(1)[0][0])\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    retrieved_labels = [birads_labels[idx] for idx in most_common_indices if idx < len(birads_labels)]\n",
    "                    precision = precision_at_k(retrieved_labels, true_label, k=k)\n",
    "                    recall = recall_at_k(retrieved_labels, true_label, total_relevant, k=k)\n",
    "                    ndcg = ndcg_at_k(retrieved_labels, true_label, k=k)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'model': model_name,\n",
    "                        'n_clusters': n_clusters,\n",
    "                        'n_bits': n_bits,\n",
    "                        'query_image': query_index,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'ndcg': ndcg,\n",
    "                        'search_time_mean': np.mean(start_times),\n",
    "                        'search_time_std': np.std(start_times),\n",
    "                        'birads_category': true_label\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error with n_clusters={n_clusters}, n_bits={n_bits}: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR,f'faiss_hyperparameter_analysis_{model_name}.csv'), index=False)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Metrics to plot\n",
    "    metrics = ['precision', 'recall', 'ndcg', 'search_time_mean']\n",
    "    titles = ['Precision', 'Recall', 'NDCG', 'Search Time (s)']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        # Group by clusters and bits\n",
    "        grouped_data = df.groupby(['n_clusters', 'n_bits'])[metric].mean().reset_index()\n",
    "        \n",
    "        # Pivot for heatmap\n",
    "        pivot_data = grouped_data.pivot(index='n_clusters', columns='n_bits', values=metric)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(pivot_data, annot=True, cmap='viridis', fmt='.4f')\n",
    "        plt.title(f'Average {title} by FAISS Parameters')\n",
    "        plt.xlabel('Number of Bits')\n",
    "        plt.ylabel('Number of Clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,f'faiss_hyperparameter_analysis_{model_name}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ====================== BIRADS Category Analysis =======================\n",
    "\n",
    "def analyze_by_birads_category(all_results, model_names, k_values):\n",
    "    \"\"\"Analyze performance metrics grouped by BIRADS category\"\"\"\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Get unique BIRADS categories\n",
    "    unique_categories = df['birads_category'].unique()\n",
    "    \n",
    "    # Create figure for plotting\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    titles = ['Precision', 'Recall', 'NDCG']\n",
    "    \n",
    "    # Plot each metric\n",
    "    for metric_idx, (metric, title) in enumerate(zip(metric_names, titles)):\n",
    "        plt.subplot(3, 1, metric_idx+1)\n",
    "        \n",
    "        for model in model_names:\n",
    "            model_data = df[df['model'] == model]\n",
    "            \n",
    "            # Calculate mean metric for each k and BIRADS category\n",
    "            category_performance = []\n",
    "            \n",
    "            for category in unique_categories:\n",
    "                for k in k_values:\n",
    "                    category_k_data = model_data[(model_data['birads_category'] == category) & \n",
    "                                                 (model_data['k'] == k)]\n",
    "                    \n",
    "                    if not category_k_data.empty:\n",
    "                        mean_metric = category_k_data[metric].mean()\n",
    "                        category_performance.append({\n",
    "                            'model': model,\n",
    "                            'category': category,\n",
    "                            'k': k,\n",
    "                            'mean_metric': mean_metric\n",
    "                        })\n",
    "            \n",
    "            # Create DataFrame for this model\n",
    "            model_perf_df = pd.DataFrame(category_performance)\n",
    "            \n",
    "            # Plot lines for each BIRADS category\n",
    "            for category in unique_categories:\n",
    "                category_data = model_perf_df[model_perf_df['category'] == category]\n",
    "                if not category_data.empty:\n",
    "                    plt.plot(category_data['k'], category_data['mean_metric'], \n",
    "                             marker='o', label=f'{model} - BIRADS {int(category)}')\n",
    "        \n",
    "        plt.title(f'{title} by BIRADS Category and k Value')\n",
    "        plt.xlabel('k Value')\n",
    "        plt.ylabel(f'Mean {title}')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,'birads_category_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create and save detailed statistics table\n",
    "    stats_rows = []\n",
    "    \n",
    "    for model in model_names:\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        for category in unique_categories:\n",
    "            category_data = model_data[model_data['birads_category'] == category]\n",
    "            \n",
    "            for k in k_values:\n",
    "                k_data = category_data[category_data['k'] == k]\n",
    "                \n",
    "                if not k_data.empty:\n",
    "                    stats_rows.append({\n",
    "                        'Model': model,\n",
    "                        'BIRADS': int(category),\n",
    "                        'k': k,\n",
    "                        'Precision': k_data['precision'].mean(),\n",
    "                        'Recall': k_data['recall'].mean(),\n",
    "                        'NDCG': k_data['ndcg'].mean(),\n",
    "                        'Search Time (s)': k_data['search_time_mean'].mean() if 'search_time_mean' in k_data.columns else k_data['search_time_sec'].mean(),\n",
    "                        'Count': len(k_data)\n",
    "                    })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_rows)\n",
    "    stats_df.to_csv(os.path.join(OUTPUT_DIR,'birads_category_detailed_stats.csv'), index=False)\n",
    "    \n",
    "    # Generate summary table (Table 2 replacement)\n",
    "    summary_table = stats_df.pivot_table(\n",
    "        index=['Model', 'BIRADS'],\n",
    "        columns=['k'],\n",
    "        values=['Precision', 'Recall', 'NDCG'],\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    summary_table.to_csv('summary_table_by_birads_and_k.csv')\n",
    "    print(\"Generated summary table to replace the missing Table 2\")\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "# ====================== Save Results to CSV =======================\n",
    "\n",
    "def save_results_to_csv(results, output_file):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# ====================== Generate Missing Table 2 =======================\n",
    "\n",
    "def generate_table_2(all_results, model_names, k_values=[10]):\n",
    "    \"\"\"Generate the missing Table 2 referenced in the paper\"\"\"\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Filter to specific k values\n",
    "    df_filtered = df[df['k'].isin(k_values)]\n",
    "    \n",
    "    # Group by model and calculate average metrics\n",
    "    table_data = []\n",
    "    \n",
    "    for model in model_names:\n",
    "        model_data = df_filtered[df_filtered['model'] == model]\n",
    "        \n",
    "        avg_precision = model_data['precision'].mean()\n",
    "        avg_recall = model_data['recall'].mean()\n",
    "        avg_ndcg = model_data['ndcg'].mean()\n",
    "        \n",
    "        if 'search_time_mean' in model_data.columns:\n",
    "            avg_search_time = model_data['search_time_mean'].mean()\n",
    "            std_search_time = model_data['search_time_std'].mean()\n",
    "        else:\n",
    "            avg_search_time = model_data['search_time_sec'].mean()\n",
    "            std_search_time = 0\n",
    "        \n",
    "        table_data.append({\n",
    "            'Model': model,\n",
    "            'Average Precision': avg_precision,\n",
    "            'Average Recall': avg_recall,\n",
    "            'Average NDCG': avg_ndcg,\n",
    "            'Average Search Time (s)': avg_search_time,\n",
    "            'Std Search Time (s)': std_search_time\n",
    "        })\n",
    "    \n",
    "    # Create and save table\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    table_df.to_csv(os.path.join(OUTPUT_DIR,'table_2_model_comparison.csv'), index=False)\n",
    "    \n",
    "    print(\"Table 2 (Model Comparison) created and saved\")\n",
    "    return table_df\n",
    "\n",
    "# ====================== Main Execution =======================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directories = ['../results/resized_low_energy', '../results/resized_subtracted']\n",
    "    \n",
    "    # List of models for comparison\n",
    "    model_names = {\n",
    "        'DenseNet121': models.densenet121(pretrained=True),\n",
    "        'ResNet50': models.resnet50(pretrained=True),\n",
    "        'VGG16': models.vgg16(pretrained=True),\n",
    "        'EfficientNet': models.efficientnet_b0(pretrained=True)\n",
    "    }\n",
    "\n",
    "    k_values = [1, 5, 10, 20, 50, 100]\n",
    "    \n",
    "    # Use more query images to address reviewer concern about sample size\n",
    "    # Select images from each BIRADS category to ensure comprehensive evaluation\n",
    "    query_indices = [0, 10, 20, 30, 50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500]\n",
    "\n",
    "    # Load BIRADS labels\n",
    "    birads_labels, birads_categories = load_birads_labels('../annotations/Radiology-manual-annotations(2).xlsx')\n",
    "    \n",
    "    # Store all results for combined analysis\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, model in model_names.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        # Modify the classifier for each model to extract embeddings\n",
    "        if model_name in ['DenseNet121', 'EfficientNet']:\n",
    "            model.classifier = torch.nn.Identity()\n",
    "        elif model_name in ['ResNet50']:\n",
    "            model.fc = torch.nn.Identity()\n",
    "        elif model_name in ['VGG16']:\n",
    "            model.classifier[6] = torch.nn.Identity()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Extract features\n",
    "        features_array, filenames, images = extract_features_from_dataset(directories, model)\n",
    "        \n",
    "        print(f\"Feature vector shape for {model_name}: {features_array.shape}\")\n",
    "\n",
    "        # Basic evaluation with L2 distance\n",
    "        results = []\n",
    "\n",
    "        for query_index in query_indices:\n",
    "            if query_index >= len(birads_labels):\n",
    "                continue\n",
    "                \n",
    "            true_label = birads_labels[query_index]\n",
    "            total_relevant = birads_labels.count(true_label)\n",
    "            birads_category = birads_categories[query_index]\n",
    "            \n",
    "            print(f\"Query {query_index}: BIRADS category {birads_category}, Total relevant: {total_relevant}\")\n",
    "            \n",
    "            for k in k_values:\n",
    "                # Build FAISS index\n",
    "                index = build_faiss_index(features_array, 'L2')\n",
    "                \n",
    "                distances, indices, search_time_mean, search_time_std = perform_faiss_search(\n",
    "                    index, query_index, features_array, k=k, distance_metric='L2', n_trials=10\n",
    "                )\n",
    "                \n",
    "                retrieved_labels = [birads_labels[idx] for idx in indices if idx < len(birads_labels)]\n",
    "\n",
    "                precision = precision_at_k(retrieved_labels, true_label, k=k)\n",
    "                recall = recall_at_k(retrieved_labels, true_label, total_relevant, k=k)\n",
    "                ndcg = ndcg_at_k(retrieved_labels, true_label, k=k)\n",
    "                rank_of_true = rank_of_true_label(retrieved_labels, true_label)\n",
    "\n",
    "                result = {\n",
    "                    'model': model_name,\n",
    "                    'query_image': query_index,\n",
    "                    'k': k,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'ndcg': ndcg,\n",
    "                    'rank_of_true_label': rank_of_true,\n",
    "                    'min_distance': min(distances),\n",
    "                    'max_distance': max(distances),\n",
    "                    'mean_distance': np.mean(distances),\n",
    "                    'search_time_mean': search_time_mean,\n",
    "                    'search_time_std': search_time_std,\n",
    "                    'birads_category': birads_category\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                all_results.append(result)\n",
    "                \n",
    "                print(f\"k={k}, Precision={precision:.4f}, Recall={recall:.4f}, NDCG={ndcg:.4f}, \"\n",
    "                      f\"Search Time={search_time_mean:.8f} Â± {search_time_std:.8f}s\")\n",
    "\n",
    "        # Save results for each model\n",
    "        save_results_to_csv(results, os.path.join(OUTPUT_DIR, f'faiss_detailed_results_{model_name}.csv'))\n",
    "        \n",
    "        # Analyze distance metrics (L2 vs Cosine)\n",
    "        if model_name in ['DenseNet121', 'ResNet50']:  # Only for selected models to save time\n",
    "            distance_results = analyze_distance_metrics(\n",
    "                features_array, birads_labels, model_name, query_indices[:5], k_values\n",
    "            )\n",
    "            all_results.extend(distance_results)\n",
    "        \n",
    "        # Analyze FAISS hyperparameters\n",
    "        if model_name == 'DenseNet121':  # Only for primary model\n",
    "            hyperparameter_results = analyze_faiss_hyperparameters(\n",
    "                features_array, birads_labels, model_name, query_indices[:3]\n",
    "            )\n",
    "            # Don't add to all_results as they use different indexing method\n",
    "        \n",
    "        # Create visualizations for each model\n",
    "        if model_name in ['DenseNet121', 'ResNet50']:  # Create for at least two models\n",
    "            print(f\"\\nCreating visualizations for {model_name}...\")\n",
    "            reduced_pca, reduced_tsne = visualize_pca_and_tsne(\n",
    "                features_array, birads_labels, model_name, n_points=500\n",
    "            )\n",
    "\n",
    "    # Analyze by BIRADS category\n",
    "    print(\"\\nAnalyzing performance by BIRADS category...\")\n",
    "    category_analysis = analyze_by_birads_category(\n",
    "        all_results, model_names.keys(), k_values\n",
    "    )\n",
    "    \n",
    "    # Generate missing Table 2\n",
    "    print(\"\\nGenerating Table 2 for the paper...\")\n",
    "    table_2 = generate_table_2(all_results, model_names.keys())\n",
    "    print(table_2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
